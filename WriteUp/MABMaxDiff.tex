%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Author template for Marketing Science (mksc)
%% Mirko Janc, Ph.D., INFORMS, mirko.janc@informs.org
%% ver. 0.95, December 2010
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass[mksc,blindrev]{informs3} % current default for manuscript submission
\documentclass[nonblindrev]{informs3}
%\documentclass[a4paper,12pt]{article}

%%\OneAndAHalfSpacedXI % current default line spacing
\OneAndAHalfSpacedXII
%%\DoubleSpacedXII
%%\DoubleSpacedXI

% If hyperref is used, dvi-to-ps driver of choice must be declared as
%   an additional option to the \documentclass. For example
%\documentclass[dvips,mksc]{informs3}      % if dvips is used
%\documentclass[dvipsone,mksc]{informs3}   % if dvipsone is used, etc.

% Private macros here (check that there is no clash with the style)
\usepackage{subcaption}
% Natbib setup for author-year style
\usepackage{natbib}
 \bibpunct[, ]{(}{)}{,}{a}{}{,}%
 \def\bibfont{\small}%
 \def\bibsep{\smallskipamount}%
 \def\bibhang{24pt}%
 \def\newblock{\ }%
 \def\BIBand{and}%

\usepackage{algorithmicx}
\usepackage{algpseudocode}


%% Setup of theorem styles. Outcomment only one. 
%% Preferred default is the first option.
\TheoremsNumberedThrough     % Preferred (Theorem 1, Lemma 1, Theorem 2)
%\TheoremsNumberedByChapter  % (Theorem 1.1, Lema 1.1, Theorem 1.2)

%% Setup of the equation numbering system. Outcomment only one.
%% Preferred default is the first option.
\EquationsNumberedThrough    % Default: (1), (2), ...
%\EquationsNumberedBySection % (1.1), (1.2), ...
\DeclareMathOperator{\logit}{logit}
% In the reviewing and copyediting stage enter the manuscript number.
%\MANUSCRIPTNO{} % When the article is logged in and DOI assigned to it,
                 %   this manuscript number is no longer necessary

%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%

% Outcomment only when entries are known. Otherwise leave as is and 
%   default values will be used.
%\setcounter{page}{1}
%\VOLUME{00}%
%\NO{0}%
%\MONTH{Xxxxx}% (month or a similar seasonal id)
%\YEAR{0000}% e.g., 2005
%\FIRSTPAGE{000}%
%\LASTPAGE{000}%
%\SHORTYEAR{00}% shortened year (two-digit)
%\ISSUE{0000} %
%\LONGFIRSTPAGE{0001} %
%\DOI{10.1287/xxxx.0000.0000}%

% Author's names for the running heads
% Sample depending on the number of authors;
% \RUNAUTHOR{Jones}
% \RUNAUTHOR{Jones and Wilson}
% \RUNAUTHOR{Jones, Miller, and Wilson}
% \RUNAUTHOR{Jones et al.} % for four or more authors
% Enter authors following the given pattern:
%\RUNAUTHOR{}

% Title or shortened title suitable for running heads. Sample:
% \RUNTITLE{}
% Enter the (shortened) title:
\RUNTITLE{Bandit MaxDiff PRELIMINARY! PLEASE DO NOT DISTRIBUTE}

% Full title. Sample:
% \TITLE{Bundling Information Goods of Decreasing Value}
% Enter the full title:
\TITLE{Bandit MaxDiff: Large-scale adaptive MaxDiff with $\epsilon$-Diffuse Thompson Sampling \\ PRELIMINARY! DO NOT DISTRIBUTE}

% Block of authors and their affiliations starts here:
% NOTE: Authors with same affiliation, if the order of authors allows, 
%   should be entered in ONE field, separated by a comma. 
%   \EMAIL field can be repeated if more than one author

%\ARTICLEAUTHORS{%
%\AUTHOR{Eric Schwartz}
%\AFF{University of Michigan, \EMAIL{ericmsch@umich.edu}, \URL{}}
%\AUTHOR{Kenneth Fairchild}
%\AFF{Sawtooth Software, \EMAIL{}, \URL{}}
%\AUTHOR{Bryan Orme}
%\AFF{Sawtooth Software, \EMAIL{}, \URL{}}
%\AUTHOR{Alexander Zaitzeff}
%\AFF{University of Michigan, \EMAIL{azaitzef@umich.edu}, \URL{}}
% Enter all authors
%} % end of the block

\ABSTRACT{%
For large MaxDiff studies whose main purpose is identifying the top few items for the sample, a new adaptive approach called Bandit MaxDiff may increase efficiency fourfold over standard non-adaptive MaxDiff.  Bandit MaxDiff leverages information from previous respondents via aggregate logit and Thompson Sampling so later respondents receive designs that oversample the topmost items that are most likely to turn out to be the overall winners. Our approach applies beyond MaxDiff problems to a more general set of bandit problems. We propose a flexible algorithm, $\epsilon$-Diffuse Thompson Sampling (TS), which nests traditional TS. Blending ideas from $\epsilon$-greedy in machine learning and Bayesian approaches, this is a more robust version of TS, which a manager can control with tuning parameters. For instance, being less risk averse, one may drawn more items from diffuse posteriors, making the algorithm robust to changing environments, even extreme non-stationarity. We implement the methods using MaxDiff survey from a large consumer packaged goods manufacturer. Beyond showing our approach outperforms better than current methods, we show under which conditions it performs even better than (larger problems) or just as well as existing methods (smaller problems).
	% Enter your abstract
}%

% Sample
%\KEYWORDS{deterministic inventory theory; infinite linear programming duality; 
%  existence of optimal policies; semi-Markov decision process; cyclic schedule}

% Fill in data.
\KEYWORDS{Active learning, adaptive conjoint, multi-armed bandit, best-worst scaling, MaxDiff, Thompson Sampling}

\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Samples of sectioning (and labeling) in MKSC
% NOTE: (1) \section and \subsection do NOT end with a period
%       (2) \subsubsection and lower need end punctuation
%       (3) capitalization is as shown (title style).
%
%\section{Introduction.}\label{intro} %%1.
%\subsection{Duality and the Classical EOQ Problem.}\label{class-EOQ} %% 1.1.
%\subsection{Outline.}\label{outline1} %% 1.2.
%\subsubsection{Cyclic Schedules for the General Deterministic SMDP.}
%  \label{cyclic-schedules} %% 1.2.1
%\section{Problem Description.}\label{problemdescription} %% 2.

% Text of your paper here
\section{Introduction}

Firms collect more customer data than ever. While much of that is behavioral data (e.g., purchases, clicks, visits, etc.), the online market research relies on surveys (e.g., preference measurement) at an increasingly large scale. And as firms push preference measurement methods into a larger scale, market researchers seek to collect data as efficiently. \\
One common managerial decision driving such data collection is to select which are most preferred among a large set of hundreds of possible items. For instance, consider these illustrative managerial decisions: select which features should we offer in a new product; select the most preferred levels of a product attribute; and select the consumer benefits we should emphasize in marketing communications.  For each problem, a market researcher may have to screen out hundreds of ``items'' (e.g., features, benefits, messages).\\
But typical choice experiment methods, such as conjoint, are infeasible for cost or simply inefficient at this scale. Instead, researchers often use pretest or screening phases to select a smaller set, and use that for the main analysis. Yet there is no systematic method to smoothly transition out of that screening phase, even when the ultimate objective is learning precisely the set of most preferred items. \\
Choice experiment methods for preference measurement continue to be among the most widely adopted market research methods. Choice-based conjoint (CBC) analysis and maximum difference (MaxDiff), for instance, are common choice tasks that continue to be used in both academic and industry settings.\\
Methodological advances, such as adaptive conjoint, have long sought to improve efficiency, obtaining more information from fewer respondents. Those methods will use past responses to select the next question to improve precision of all parameters. For instance, adaptive choice-based conjoint (ACBC) methods present respondents with questions to reduce the uncertainty where it is greatest. It chooses the next question to best reduce overall uncertainty.\\
But the market researcher's goal is typically not to obtain precise estimates of utility of every single alternative or attribute level. If their goal is to identify the most preferred items, why do they need to spend resources (questions, respondents, time) to learn how precisely poor the least-preferred item is compared to the second-least preferred item? \\
With that objective in mind, we propose a new adaptive choice experiment method. 
Our approach departs from traditional adaptive survey methods like adaptive conjoint analysis (ACA). They aim to improve the precision of each of the parameters; we estimate the best items more precisely. \\
We propose a new adaptive method, combining MaxDiff and multi-armed bandit methods, as an example of an even broader framework for stated choice data collection. The focus of our research is a broader framework to align the data collection process with such managerial objectives. By aligning the two, we aim to improve efficiency and scalability for big data collection via choice experiments. We find we can ``get more for less,'' by improving precision using a smaller sample size compared to existing methods.\\
We propose this framework more broadly but will demonstrate it with MaxDiff, as it is an increasingly popular and important choice experiment method. But we will consider the general choice-based conjoint case in our discussion section. \\
Our approach draws on state-of-the-art MAB algorithms in statistical machine learning called Follow The Perturbed Leader (FTPL)~\cite{kalai2005efficient}. Recent theoretical advances prove that adding perturbations of Gumbel distributed noise leads to an optimal MAB strategy~\cite{abernethy2015fighting};~\cite{kujala2005following}. The perturbation methods provide an intuitive randomization decision strategy, which is appropriate for our application, yet their connection to other approaches and applications are limited. Perturbation not only resolves the explore-exploit (learn-earn) tradeoff for the stochastic (iid) bandit setting, but it also does so in the adversarial setting, suggesting it is an MAB strategy more robust to changes in the non-random changes in the environment. We shed light on this methodological link as it has consequences beyond our method.\\
We contribute three ideas to the literature and practice. First, we frame choice task data collection as an earning-and-learning problem. During the process of data collection, we simultaneously balance the desire to learn preferences of all items and earn a reward towards achieving objective, by only serving questions the truly best subset of items. In this case, the objective is to identify the best set of items, for instance, the most desirable levels of attributes. The faster we identify the best items, the more we include them in questions, improving the precision of estimates. And if we need to reach a desired level of precision, we can do so faster, saving money in the form of number of respondents and time. %The precedeing paragraph needs to be changed. We are not doing any earning just learning Pure exploration
Multi-armed bandit problems appear in marketing experiments for online advertising~\cite{schwartz2015customer},~\cite{urban2013morphing} and website design~\cite{hauser2009website}. One particular MAB method amenable to multi-variable parametric models is Thompson Sampling.\\
While we use bandit algorithms, our approach differs from extant bandit applications in a number of ways. For a bandit problem the reward is observed immediately (click, acquisition, purchase). While we have immediate observations (choice), that is not our reward. Instead, our goal is to correctly identify the truly best items as precisely as possible. These are not evaluable in real-time. \\
While our problem is not a bandit problem exactly, it is a bandit-inspired problem. However we find bandit algorithms form a heuristic to achieve our objective better than existing MaxDiff and conjoint methods. \\
Consider what would occur if the reward was whether an item is selected as best alternative, the arm's reward would be completely dependent on all of the other items presented in the set (e.g., was it presented alongside poor items). 
Second, the need to optimize this process is greater for large-scale problems with extremely large number of possible attribute levels or items.  In settings with hundreds of items, there is greater opportunity cost of focusing on items that are not important, calling for a need to improve efficiency.\\
Third, we propose an adaptive method for best-worst scaling. Existing adaptive methods have been largely limited to conjoint, adapting at both the aggregate level,~\cite{arora2001improving}, and the individual level,~\cite{toubia2004polyhedral}. Yet best-worst methods, such as MaxDiff, have continued to emerge as important and commonly used in areas including marketing research and public health. We introduce a first step to make MaxDiff adaptive in a principled manner by using MAB methods. We accommodate an increasingly accepted method of solving multi-armed bandit problems, Thompson Sampling. We introduce two versions: MaxDiff-Thompson Sampling and its generalization MaxDiff $\epsilon$-diffuse Thompson Sampling.\\
The $\epsilon$-diffuse Thompson Sampling nests traditional TS. We combine ideas from $\epsilon$-greedy in machine learning and Bayesian approaches to produce a more robust version of TS,  which a manager can control with tuning parameters. For instance, if a manager wants to be extra conservative and maintaining more samples drawn from diffuse posteriors, she will make the algorithm robust to changing environments, even extreme non-stationarity. \\
The rest of the paper is structured as follows. In the next section we introduce the illustrative problem of maximum difference scaling focusing on its use in practice with large numbers of items. We then briefly introduce the multi-armed bandit framework. After formalizing the discrete choice model of maxdiff analysis, we introduce our two algorithms that implement this method for Maxdiff Surveys. Then we present our empirical results in regular setting and then present variants of our main results. We implement the methods using MaxDiff survey implemented by Sawtooth Software with a large consumer packaged goods manufacturer. \\


\section{Framework and literature}
\subsection{Maximum Difference Scaling}
\begin{figure}
\caption{MaxDiff is becoming more popular over time with Sawtooth Software users}
\includegraphics[width=0.5\textwidth]{plots/maxdiffpop}
\label{fig:pop}
\end{figure}
MaxDiff, Maximum Difference Scaling, is a preference measurement and item scaling method.  In a MaxDiff questionnaire the researcher asks respondents for their best and worst item out of a set, then repeats this choice task. Initially proposed by~\cite{louviere1991best}, MaxDiff was first released as a software system in 2004 by Sawtooth Software, a top marketing research software company in North America. Since its release its popularity increased steadily with penetration of the technique now reaching 68\% of all Sawtooth users in 2015 (Figure \ref{fig:pop}). \\
MaxDiff offers benefits over alternative methods. MaxDiff provides more discrimination among items and between respondents on the items than traditional rating scales~\cite{cohen2004s}. Besides enhanced discrimination, it avoids the scale use bias so problematic with traditional ratings scales (CITE).\\
MaxDiff, while it is a distinct type of choice experiment, is closely related to conjoint. A key difference is that it involves both best and worst choices instead of only one. But in its most common form, MaxDiff may be thought of as a one-attribute CBC study with many levels.  \\
\subsection{Studying More Items with MaxDiff: Survey of Market Research Practitioners}
MaxDiff has proven so useful that market researchers increasingly find reasons to use MaxDiff for a large number of items.  How many is a large number of items?  In their 2007 paper, Hendrix and Drucker described ``large sets'' as about 40 to 60 items, proposing variants to MaxDiff called Augmented and Tailored MaxDiff to handle such large problems~\cite{hendrix2007alternative}. In their 2012 paper, Wirth and Wolfrath also investigated variants to MaxDiff called Express and Sparse MaxDiff for handling what they described as ``very large sets'' of items~\cite{wirth2012largeset}.  Very large to these authors meant potentially more than 100 items.  To support their findings, they conducted a study among synthetic robotic respondents with 120 items and a real study among humans with 60 items.\\
For Hendrix and Drucker 40 to 60 items was large, for Wirth and Wolfrath 120 items was very large.  For this current paper, we're referring to huge numbers of items as potentially 300 or more. The motivation of our research is beyond academic curiosity, as marketing researchers are seeking such applications pushing MaxDiff further than it was perhaps ever intended. \\
\begin{figure}
\caption{Maximum Number of Items Studied via MaxDiff over Last 12 Months}
\includegraphics[width=0.5\textwidth]{plots/maxnumstudy}\\
N = ??, Mean= 40, Median=30, Maximum=400
\label{fig:max}
\end{figure}
In Sawtooth Software's 2015 Customer Feedback Survey, we asked respondents to tell us the largest number of items they had included in a MaxDiff study during the last 12 months (Figure \ref{fig:max}). Nearly one-fifth of respondents indicated their firms had conducted a study with 51 or more items.  The maximum number of items studied was 400!\\
To some it may seem bizarre and overwhelming that some researchers are conducting MaxDiff studies with 81+ or even 400 items.  However, when we consider that individual MaxDiff items may actually represent conjoined elements that constitute a profile (say, a combination of packaging style, color, claims, and highlighted ingredients), then it can make much more sense to do 400-item studies.  If the profiles involve multiple highly interactive attributes that pose challenges for CBC, then MaxDiff with huge numbers of items could be a viable alternative (given the new approach we demonstrate further below).\\
\begin{figure}
\caption{Main Purpose for MaxDiff Study with 41+ Items}
\includegraphics[width=0.5\textwidth]{plots/maxdiffpurpose}
\label{fig:purpose}
\end{figure}
We also asked Sawtooth Software customers what the main purpose was for that study with the reported maximum number of items.  For studies involving 41 or more items, the main reasons are displayed in Figure 3.\\
For 42\% of these large MaxDiff studies, the main purpose was to identify the TOP item or TOP few items.  Our research shows that if this is the main goal, then traditional design strategies are very wasteful.  An adaptive approach using Thompson Sampling can be about 4x more efficient.  Without the Thompson Sampling approach, you are potentially wasting 75 cents of every dollar you are spending on MaxDiff data collection. \\
The problem is that current MaxDiff approaches don't scale well to increasing the number of items.  More items require commensurately longer questionnaires, larger sample sizes, and larger data collection costs with more tired respondents.  If the researcher is concerned about obtaining robust individual-level estimates for all the items, then the current methodologies especially don't scale well to large lists of items.  Respondents just tire out with such long surveys.  In contrast, our approach employs an adaptive divide-and-conquer aggregate approach that leverages prior learning to create more efficient questionnaires and more precise aggregate score estimates. \\
\subsection{Overview of approach: Bandit MaxDiff with Thompson Sampling}
Thompson Sampling has been proposed as an efficient solution for solving the multi-armed bandit problem. Thompson Sampling involves allocating resources to an action in proportion to the probability that it is the best action~\cite{thompson1933likelihood}. Any bandit method must find an appropriate balance between exploring to gain information and exploiting that knowledge.\\
On the one hand, we want to learn about the relative scores of a large number of items within a MaxDiff problem. On the other hand, we want to utilize what we have learned so far to focus our efforts on a targeted set of actions that will likely yield greater precision regarding the items of most interest to the researcher. While there are many methods to accomplish this, Thompson Sampling has proven very useful for these types of problems. For a marketing application of Thompson Sampling and a review of the literature, see Schwartz et al. (2016). \\
The traditional MaxDiff design approach shows each item an equal number of times across all respondents x tasks.  However, if the main goal is to identify the top few items for the sample, after the first, say 20, respondents it seems reasonable to start paying attention to the already-collected MaxDiff responses and oversampling the items that are already viewed as most preferred (the stars).  We can use aggregate logit to estimate both preference scores and standard errors at any point during data collection (say, after the 20th, 40th, $\ldots$. respondent has completed the survey).\\
Thompson Sampling makes a new draw from the vector of item preferences using the estimated population preferences (aggregate logit scores) plus normally distributed error, with standard deviations equal to the standard errors of the logit weights.  As the sample size increases, the standard errors of course tighten.\\
\begin{figure}[!ht]
\caption{Respondent-by-item counts}
\includegraphics[width=1\textwidth]{plots/3dotplot.pdf}
\label{fig:dots}
\end{figure}
To understand the logic of the algorithm, consider a snapshot in time during the data collection. Imagine we have just collected data from 100 respondents, and we decide to summarize their preferences (for each of 100+ items) with aggregate logit.  Then, to generate a MaxDiff task for the 101st respondent, we could generate a draw from the population preferences leveraging the population means and normal errors with standard deviations equal to the empirically estimated standard errors.  We then can sort that newly sampled vector of preference scores from the most to the least preferred item.  The five most preferred items might be taken into the first task to show to the 101st respondent.  The process (with or without updating the logit weights after recording the first task's answer) could be repeated to choose the five items to show in the second task for the 101st respondent, etc.  To reduce the load on the server managing the data collection, perhaps only after every 20th respondent has completed the survey, the logit weights and standard errors would be updated.\\
We note that the translation of MaxDiff into a bandit problem is not obvious. Our goal is identifying the utilities of the top set of items with maximum precision. Selecting an arm corresponds to including it in the survey for the next respondent. The reward, however, is not as clear. On the one hand, suppose each task included all possible items. Then the reward would be clear: whether the item is chosen as the ``best.'' But it is not feasible to show so many items to respondents, so that's not a relevant scenario. On the other hand, given we only select a subset of items, the item receiving the ``best'' label doesn't translate directly to a reward. Indirectly, that choice data does enable us to infer how it ranks among all items, exactly aligning with our goal. We use the alignment of the managerial goal and the MAB algorithm's balancing of learning and earning to our advantage.
%Is our goal to identify top items with maximum precision? Because we do not show any data that we do that or that is even important for problem (identify top items)
\section{Specifically Bandit + Conjoint = Thompsons + MaxDiff}
We formalize the proposed procedure in three stages. First, we introduce MaxDiff as a best-worst scaling discrete choice task, and we relate it to the standard multinomial logit model. Next, we introduce Thompson Sampling as the natural method of resolving the learn-vs-earn tradeoff as in a multi-armed bandit problem.  Then we generalize that Thompson Sampling algorithm to be more robust by sampling a proportion of items from a more diffuse distribution.
\subsection{MaxDiff choice model}
Every respondent selects both the best and the worst option from an available set of options in each discrete choice task. The model for that data comes from a class of probability models known as best-worst scaling, and MaxDiff is one such model. We adopt the framework from the best-worst scaling literature. For a review, see ~\cite{marley2012models} Marley 2010, ~\cite{marley2005some}. 
We have K possible items, and we select a set S for each choice task. To describe the items, we define two random variables, best $B_z$ and worst $W_z$, for each $z \in S$.  We then define a third random variable, best-worst $BW_{r,s}$, for any $r,s \in S$.\\
Following a random utility framework, these utilities have deterministic and stochastic components.\\ 
A consistent extreme value random utility model is a Thurstone random utility model where each $\varepsilon_z$ has the extreme value distribution. Consistent model means $B_z=-W_z=U_z$ and $BW_{r,s}=U_r-U_s$, then
\begin{align*}
&B_z=v_z+\varepsilon_z\\
&W_z=-v_z-\varepsilon_z\\
&BW_{r,s}=v_r-v_s+\varepsilon_r-\varepsilon_s\\
\end{align*}
We can write the probability that an item is the best and the probability that the item is the worst.
\begin{align*}
&B_S (x)= Pr⁡( B_x=\max_{z \in S} B_z)\\
&W_S (y)= Pr⁡( W_y=\max_{z \in S} W_z)\\
\end{align*}
Without even using the particular utility or scale of items we can derive choice probabilities. In the most general, form we suppose $b()$ and $w()$ are separate interval scales. Then the resulting probability of an item being best or worst is
\begin{align*}
&B_S (x)= \frac{b(x)}{\sum_{z \in S}b(z)}\\
&W_S (y)= \frac{w(y)}{\sum_{z \in S}w(z)}\\
\end{align*}
For any pair of items, $x,y \in S$, we can write the joint probability that $x$ is best and $y$ is worst. However, when considering the best and worst jointly the scales $b$ and $w$ are not separately identified. So we fix their ratio for the same item by setting $w(z)=\frac{c}{b(z)}$. The resulting joint probability is a function of the ratios of scales for pairs of items, 
\begin{align*}
&BW_S(x,y)=Pr(U_x>U_z>U_y | z \in S -\{x,y\}, x\neq y)\\
&BW_S(x,y)=\frac{b(x)/b(y)}{\sum_{r,s \in S, r \neq s}b(r)/b(s)}
\end{align*}
To accommodate the standard utility structure, we let utility $u(z)=\log{(b(z))}$ Then each of the probabilities 
\begin{align*}
&B_S(x)=\frac{e^{u(x)}}{\sum_{z \in S} e^{u(z)}}\\
&W_S(y)=\frac{e^{u(y)}}{\sum_{z \in S} e^{u(z)}}\\
&WB_S(x,y)=\frac{e^{u(x)-u(y)}}{\sum_{r,s \in S, r\neq s} e^{u(r)-u(s)}}
\end{align*}
We can derive the same representation from the random utility model~\cite{marley2005some}. \\
We can view best-worst choice as a generalization of the classic multinomial logit for the choice of the best only.\\
\subsection{Estimation}
From our perspective as researchers, all of the utilities are unknown. We can estimate the MaxDiff choice model in a variety of ways. One way to do this exactly is to enumerate all possible pairs of items $x$ and $y$ and then we describe their joint probability of being best and worst, which is the probability of being having the largest difference $BW_S(x,y)$. The pairwise approach scales quadratically in the number of items. We adopt an alternative approach, which reflects the literature and practice and is shown to be a near exact approximation ~\cite{cohen2003maximum}. This allows us to estimate the best model and worst model independently, without explicitly estimating the best-worst probability. 
We describe the data for any individual-task combination. Let $Y_{B_S}(z)$ be the binary choice variable, which equals 1 if the item is selected as best in the set $S$, and 0 otherwise. Then $Y_{W_S}(z)$ is the indicator of whether item $z \in S$ is selected as the worst. The design matrix $X_{B_S}$ (of size $|S|$-by-$N$) contains indicator variables taking on value of 1 for each item in the current set $S$ and 0 otherwise. To signal the item as worst, we set $X_{W_S}=-X_{B_S}$, so $X_{W_S}$ contains values of 0 or -1.  Taken together, we express the negative loglikihood of the choice data as a multinomial logit with choice probabilities in vector notation as follows,
\[
-\frac{\exp{(\begin{bmatrix}Y_B\\Y_W\end{bmatrix}\theta)}}{\exp{(\begin{bmatrix}X_B\\X_W\end{bmatrix}\theta)}}
\]
The rows in this matrix representation represent every respondent-task-item combination, $N*J*|S|$, repeated twice.  
The link between the models of best choice and worst choice is the parameter $\theta=\{\theta_1,\ldots,\theta_k \}$. This common parameter vector represents the overall utility of each item $1,\ldots,k$. For a more positive $\theta_k$, the item $k$ has a larger probability of being chosen as best; the more negative, the more likely the item will be chosen as worst.\\
We clarify language about utility, which may diverge from conjoint language or language for multi-attribute profiles. Since $X$ is an indicator, the $\theta$ only represents the utility of item $k$ being included versus excluded. If $\theta_k > \theta_{k'}$, then we say item $k$ is ``more preferred,'' ``more important,'' or simply, ``better.''\\
Due to the sparse nature of MaxDiff for huge numbers of items plus the desire for rapid real time updates, we decided to use aggregate MNL rather than a Bayesian approach.\\
The loglikihood can be written in summation notation where $S_n$ denotes the nth set of choices as follows
\[
LL(\theta)=-\sum_{n=1}^N \sum_{x \in S_n} (Y_{B_{S_n}}(x)\log{\frac{e^{\theta_x}}{\sum_{z\in S_n} e^{\theta_z}}}+ Y_{W_{S_n}}(x)\log{\frac{e^{-\theta_x}}{\sum_{z\in S_n} e^{-\theta_z}}})
\]
In our work we find $\theta$ by minimizing the negative loglikihood using Newton-Rapson.

\subsection{Adaptive MaxDiff}
Since we are uncertain about the each parameter value, we continue collecting data. But we do not need to reduce that uncertainty equally for each one.  To translate our current beliefs about parameters into action, we use Thompson Sampling via Bayesian Boostrap. The following sections descirbe sampling scemes that use draws from the posterior to select which items to present to the next respondent. A draw $u$ is made in the following way.
\begin{align*}
&\beta_1,\beta_2,\ldots,\beta_N \sim \text{exp}(1)\\
&LL(\theta;\beta)=-\sum_{n=1}^N \beta_n\sum_{x \in S_n} (Y_{B_{S_n}}(x)\log{\frac{e^{\theta_x}}{\sum_{z\in S_n} e^{\theta_z}}}+ Y_{W_{S_n}}(x)\log{\frac{e^{-\theta_x}}{\sum_{z\in S_n} e^{-\theta_z}}})\\
&u=\argmin_{\theta} LL(\theta;\beta)
\end{align*}

Given a sampling scheme we serve up questions in the following way.

\textbf{BEGIN:}
\begin{enumerate}
\item For respondents n =1,\ldots,initial
\begin{enumerate}
\item \textbf{Act}: select items for MaxDiff design
\begin{enumerate}
\item Sample $L$ items from the population 
\item Create and serve MaxDiff questionnaire to next respondent $J$ tasks of $|S|$ Qs=MDDesign($L$,$|S|$)
\end{enumerate}
\end{enumerate}
\item For respondents n = initial + 1,\ldots, N
\begin{enumerate}
\item \textbf{Draw}: Draw from the posterior via Bayesian Bootstap $u_1,\ldots,u_n$

\item \textbf{Value}: Choose $L$ items by sampling rule
\item \textbf{Act}: select items for MaxDiff design
\begin{enumerate}
\item Create and serve MaxDiff questionnaire to next respondent $J$ tasks of $|S|$ Qs=MDDesign($L$,$|S|$)
\end{enumerate}
\end{enumerate}
\end{enumerate}
\textbf{END ALGORITHM}


%Discuss relation to the Follow the regularized leader / perturbed leader 
\begin{table}[ht]
\caption{$O(w)$ on 24 slices of simulated data with $w=6$}
\begin{tabular}{p{5cm}|p{11cm}}
Algorithm & Short Description \\
\hline
MaxDiff TS & Sample from the current posterior distribution and serve up the $L$ items with the highest sampled utility.\\
MaxDiff $\epsilon$-Diffuse TS & Sample from the current posterior distribution serve up the $(1-\epsilon)L$ items with the highest sampled utility and sample from the current diffuse posterior distribution and serve up the $\epsilon L$ items with the highest sampled utility not in $(1-\epsilon)L$.\\
$\epsilon$-greedy & Take the $(1-\epsilon)L$ with the current estimated $\theta$. Take the remaining $\epsilon L$ uniformily from the remaining items.\\
$\epsilon$-Diffuse TS closest to the threshold & Take the $(1-\epsilon)L$ and  $\epsilon L$ items that have sampled utility closest to $\frac{\theta_k+\theta_{k+1}}{2}$.\\
$\epsilon$-greedy closest to the threshold & Take the $(1-\epsilon)L$ items that have estimated utility closest to $\frac{\theta_k+\theta_{k+1}}{2}$ and $\epsilon L$ uniformly from the remaining.\\
Misclassification Minimization with random perturbation& Take the $L$ items that have most likely been misclassified (bottom items that should be top items and vice-versa). Add perturbation to the that probability.\\
Greatest Uncertainty with random perturbation& Take the $L$ items whose probabilities of being a top item are closest to 50\%. Add perturbation to the that probability.\\

\end{tabular}
\end{table}

\subsection{Adaptive Methods}

After an initial number of respondents, we begin adapting. We update beliefs about the model parameters. We can draw from an exact or approximate posterior distribution. \footnote{In the case of the multinomial logit, we can use MCMC to obtain samples, sample from asymptotic distribution via MLE implied by the estimated mean and standard errors, or as we do in the empirical application, use Bayesian bootstrapping.}\\
From a single sample of parameter values, we rank the items by utility. We select the top $L$ items, and we generate a MaxDiff design of $J$ best-worst tasks, each with a choice set of $|S|$ items.  We use $L=20$, $J=12$, $|S|=5$ as a default since it is a standard number of items used in MaxDiff studies ~\cite{wirth2012largeset}. 
The algorithm learns over time to intuitively achieve the goal of identifying the items with truly high utility with high precision. Early on, we still have substantial uncertainty. The independent samples will differ substantially in rank order of item utilities. This yields MaxDiff designs across respondents with less overlap in items. Later, the uncertainty is reduced most around the truly high-utility items. Across independent samples, the ranking of items will be highly correlated near the top of the ranking (but not near the bottom where uncertainty remains large). As a result, the top subset of $J$ items selected converges to the same group for each respondent. \\
There is one practical issues we highlight. We want to avoid repeating extremely similar questions to the same respondent. A natural consequence of Thompson Sampling is convergence: as the sample size grows, items that are most preferred by the population will achieve high preference scores with smaller standard errors.  Without any additional restrictions, the same few items will eventually tend to be drawn into adjacent MaxDiff tasks for the same respondent, causing much annoyance due to the severe degree of item repetition.  Although this is statistically most efficient, it would drive human respondents mad.  To avoid this, we use Thompson Sampling to draw a fixed number of items (e.g., 20 or 30) to show each respondent.  Those draws of 20 items are shown to each respondent in a balanced, near-orthogonal design, leading to a palatably low degree of repetition of items across adjacent sets.  The attentive reader will notice that our approach is quite similar to Wirth's Express MaxDiff approach, except that the logic for selecting the 20 items for each respondent is adaptive, using Thompson Sampling, leveraging information from the previous respondents-focusing the most recent respondent's efforts on discriminating among items that already have been judged likely to be the stars.\\
One issue this approach avoids is the respondent tiring out. Instead of trying to create a MaxDiff design with 120 items, which would lead to an unreasonable number of best-worst tasks per respondent, we use the smaller subset of 20.  We also avoid repetition. A particularly annoying alternative is to draw an independent Thompson samples ranking for each choice task for the same respondent. This results in very repetitive choice tasks as parameter estimates. \\


\subsubsection{Algorithm MaxDiff TS}
While typical Bayesian or numerical integration methods call for large numbers of draws (or sufficient number) to achieve coverage of the full distribution of parameter values, our  approach relies on the variability of a single sample. There is value in the sample to sample differences in the value of parameters and their relative rank ordering. It allows  \\
One practical issue with Thompson Sampling is robustness to changes overtime. On the one hand, there is built-in robustness. Recall the algorithm is stochastic and adapts continuously. If the early data leads the algorithm astray, then it will self-correct, eventually finding and converging to the truly best items. Suppose the early respondents made choices, by chance, leading us to believe certain items were the best when they were not. The respondents that immediately follow will start receiving these truly poor items. \\
However, as this continues, the uncertainty is reduced around these poor items to reveal there are many other items with probability of being better. By sampling from the joint belief distribution of item utilities, we will be less likely to draw those poor items. One concern is that such sampling could be too aggressive.\\

\subsection{Variant MaxDiff $\epsilon$-Diffuse TS}
Perhaps the natural parameter uncertainty is not enough or is perhaps too slow to adjust, so we propose an algorithm MaxDiff epsilon-diffuse TS, which has an extra layer of self-correction, making it more robust to non-stationarity or respondent self-selection. 
The way the $\epsilon$-$\delta$ version hedges its bets on the best items intuitively. As in illustration, instead of sampling 20 items from TS, we use 15 of the items drawn using standard Thompson Sampling (L=20, $\epsilon$=1/4), and 5 of the items drawn using Thompson Sampling with a much more diffuse prior (subsampling $\delta$=.25 of the data with replacement and drawing $u$ as a Bayes bootstrap of the subsampled data).
For $\epsilon$-$\delta$ case, its density does not become a spike as quickly as it does without $\epsilon$-$\delta$ . This speed is controlled through two parameters: epsilon, the proportion of items sampled from the diffuse distribution, and $\delta$, factor increasing variance for the diffuse distribution.  The larger the $\epsilon$ and $\delta$, the more exploration and slower the algorithm settles on its set of items. The epsilon comes from the popular $\epsilon$-greedy.
The $\epsilon$-$\delta$  generalization is just like a two-component mixture model, which nests a pooled model without segments. As the epsilon is zero, the diffuse distribution is never used, so the algorithm collapses to regular TS. As $\delta$ is 1, the diffuse distribution is equivalent to the non-diffuse distribution.
In ~\cite{toubia2007adaptive} a similar idea is used to avoid misclassifying an item that will not be sampled further. The author derived insight from genetic algorithms where ``decreasing the mutation rate over the number of iterations (or generations) leads to higher performance compared to a fixed mutation rate.'' This suggests sampling from a diffuse distribution gives benefit over uniform sampling over the remaining items as the variance of the diffuse distributions shrink with the number of respondents interviewed.



\subsubsection{Algorithm $\epsilon$-greedy}
Let $\beta$ be the current estimated parameters. Take the top $((1-\epsilon)L)$ and choose the remaining $(\epsilon L)$ uniformly from the remaining items.

\subsubsection{Variant Closest to the Threshold}
In ~\cite{toubia2007adaptive},  They introduce Closest to Threshold based on Bradlow and Wainer’s (1998) recommendation
(``select ideas near the cutoff''). We also we do that to find the top $k$. Based on the estimated utilities calculate the cutoff $c=\frac{\theta_k+\theta_{k+1}}{2}$ then for a draw $u$ calculate a score $s_i=|c-u^i|$ for every item. Then take the $L$ items with the lowest score. Similarily as before we can define $\epsilon$  diffuse version where. We have two draws $u_R$ and $u^D$ and from those scores $s_R$ and $s_D$ and Combine diffuse and regular samples into $L$ items. (Remove duplicates items. Take the next best unique item from the diffuse sample not already selected.) You can also do the $\epsilon$ - greedy version where the scores are $s_i=|c-\beta^i|$ you take the $((1-\epsilon)L)$ lowest scores and the rest uniformly.

\subsubsection{Algorithm Misclassification Minimization}
Also taken from ~\cite{toubia2007adaptive}. You define a score, $s_i$ for each item as follows. If the $i$th item is estimated to be a bottom item the score is $s_i=\mathbb{P}(\text{item i is in the top k})$. If the $i$th  item is estimated to be in the top $k$ then the score is $s_i=\mathbb{P}(\text{item i is not in the top k})$. You can estimate this quanity by taking a large number of draws from the posterior distribution. You take the $L$ items with the highest score.

\subsubsection{Algorithm Greatest Uncertianty}
You define the score, $s_i$ for each item as follows. $s_i=|.5-\mathbb{P}(\text{item i is in the top k})|$. You can estimate this quanity by taking a large number of draws from the posterior distribution. You take the $L$ items with the lowest score.

\subsubsection{Variant Random Pertubation}
Using bayes bootstrapping for estimating probabities required in Misclassification Minimization and Greatest Uncertianty takes a good deal of time. As an alternative you can estimate the scores once every $b$ people, Let us call $b$ the batch size. And then for each person perturb the scores by a normal vector with mean zero and varaince $c\frac{1}{n_i}$, Where $n_i$ is the number of questions that has had item $i$ and $c$ is some constant, we take it to be $\frac{1}{10}$. Serve up the $L$ items accordingly.   

\subsection{Main Empirical Analysis}
We compared our proposed bandit MaxDiff approaches to existing non-adaptive fixed MaxDiff design strategies using simulation based on inferred preferences from an actual MaxDiff survey. We used MaxDiff data from a survey conducted by Procter \& Gamble with Sawtooth Software. The study involved 981 respondents and 120 items from a sparse MaxDiff study. The subject matter and exact item text was hidden for confidentiality purposes. They provided us with the individual-level posterior mean utilities for all respondents and items, which were obtained via Markov Chain Monte Carlo sampling for a hierarchical Bayes (HB) logit model. We call those individual-level utilities the true HB utilities.  These HB utilities offered realistic patterns of preferences across the items and respondents for use in our respondent simulations.  Our simulated respondents mimicked the actual respondents' preferences, on average: to answer each new MaxDiff task, we perturbed those true HB utilities by iid Gumbel error.  \\
For each sample of simulated respondents, we ran aggregate logit and compared the current rank order of the estimated aggregate item scores to the true rank order for the known true utilities.  We define hit rate to reflect how accurately we could recover the top few items as observed in the true preferences for the Procter \& Gamble dataset.  To stabilize the hit rate results (since there was a random component to the responses), we ran the simulations each 100 of times. \\
We used two hit rate measures.\\ \textbf{Top 3 hit rate}: what percent of the top 3 true items the estimated scores using robotic respondents identified.\\ \textbf{Top 10 hit rate}: what percent of the top 10 true items the estimated scores using robotic respondents identified.  For instance, if the estimated scores identified 7 of the true top 10 items (irrespective of order), the hit rate was 70\%.\\
Hit rate is analogous to regret in a typical bandit problem. It measures how far we are from always selecting the truly best arm for all time periods. Although it is not exactly an observed reward in the sense of the usual bandit setting (e.g., clicks, purchases), this serves as the measure of success, \textit{ex post}.\\
\begin{figure}[!ht]
\caption{Distrubution of True Utilities}
\includegraphics[width=1\textwidth]{plots/utilscore.pdf}
\label{fig:util}
\end{figure}
The difficulty of identifying the truly best items is related to the differences among those top item utilities. With 120 items in the dataset, it shouldn't surprise us if the true preferences for the top 15 or so items were very close in terms of utility.  We certainly observed that with this dataset (Figure \ref{fig:util}).  Due to how tightly the top items in preferences clustered (there were no runaway winners), the hit rate measures we employed were quite discriminating between competing methods.\\
Using bootstrap sampling (sampling with replacement), we simulated the process of collecting respondent data up to sample sizes of $N=500$.  We considered the first 20 respondents to be the initial group, and all adaptive methods only begin after the 20th respondent. Each robotic respondent completed 12 choice sets, where each set included 5 items, which was viewed as fairly typical of larger MaxDiff studies in practice.
The MaxDiff approaches we tested were the following. For our actual implementation of Bandit MaxDiff, we used Thompson Sampling to draw 20 of the 120 items for each respondent (tending to oversample the ``stars'' based on aggregate logit estimates from previous respondents).  As described earlier, we tested two different Bandit Maxdiff approaches:\\
TO DO - maybe put new algorithmic ideas here
Bandit MaxDiff TS: All 30 items drawn via standard Thompson Sampling.\\
Bandit MaxDiff epsilon-diffuse TS: We begin with $\frac{1}{4}-\frac{3}{4}$ split ($\epsilon=\frac{1}{4}$, $\delta$=.25). So 20 of the items drawn using standard Thompson Sampling; 10 of the items drawn using Thompson Sampling with a much more diffuse prior (standard errors multiplied by 10). We also tested a pair of existing fixed MaxDiff approaches.\\
\textbf{Fixed Express MaxDiff}: we randomly drew 20 of the 120 items to show to each respondent.  Each item appeared $\frac{12*5}{20} = 3$ times per respondent.  Across respondents, each item appeared the same number of times. We primarily use Express MaxDiff as our base case.\\

\begin{figure}
\caption{3 Hit Rate with 120 items}
\includegraphics[width=1\textwidth]{plots/hr120v20k3.pdf}
\label{fig:3hit}
\end{figure}
\begin{figure}
\caption{10 Hit Rate with 120 items}
\includegraphics[width=1\textwidth]{plots/hr120v20k10.pdf}
\label{fig:10hit}
\end{figure}
\begin{figure}
\caption{20 Hit Rate with 120 items}
\includegraphics[width=1\textwidth]{plots/hr120v20k20.pdf}
\label{fig:20hit}
\end{figure}
\begin{figure}
\caption{40 Hit Rate with 120 items}
\includegraphics[width=1\textwidth]{plots/hr120v20k40.pdf}
\label{fig:40hit}
\end{figure}
Figures \ref{fig:3hit}, \ref{fig:10hit}, \ref{fig:20hit}, and \ref{fig:40hit} show the results for the 120-item dataset.  The X-axis indicates the number of cumulative respondents interviewed, and the Y-axis reports the hit rate obtained at each cumulative sample size.  For example, after the first 200 respondents, the Fixed Express Design obtains a hit rate of 60\% whereas the other approaches achieve a hit rate of about 80\%.\\
The key takeaway from Figure \ref{fig:3hit} is simple: any adaptive method is about 4x more efficient than the standard Fixed Express MaxDiff approach.
\begin{table}
\begin{center}
\begin{tabular}{llllllllll}
\hline   k &  fixed\_express &  greedy &  greedythres &  mismin &    TS &  TSe4 &  TSthres &  uncert \\ \hline    3 &          0.650 &   0.853 &        0.860 &   0.883 & 0.863 & 0.870 &    0.867 &   0.840 \\  10 &          0.825 &   0.880 &        0.902 &   0.907 & 0.923 & 0.912 &    0.911 &   0.901 \\  20 &          0.824 &   0.784 &        0.863 &   0.885 & 0.825 & 0.821 &    0.867 &   0.886 \\  40 &          0.857 &   NA &        0.884 &   0.908 & NA & NA &    0.896 &   0.897 \end{tabular}
\end{center}
\caption{Top k Hit Rate for Various Algorithms at the 260th Respondant}
\label{table:at260}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{llllllllll}
\hline   k &  fixed\_express &  greedy &  greedythres &  mismin &    TS &  TSe4 &  TSthres &  uncert \\\hline   3 &          0.780 &   0.950 &        0.933 &   0.950 & 0.940 & 0.947 &    0.963 &   0.943 \\  10 &          0.877 &   0.925 &        0.928 &   0.939 & 0.933 & 0.941 &    0.943 &   0.945 \\  20 &          0.862 &   0.827 &        0.914 &   0.928 & 0.863 & 0.864 &    0.913 &   0.919 \\  40 &          0.887 &   NA &        0.926 &   0.937 & NA & NA &    0.930 &   0.938 \end{tabular}
\end{center}
\caption{Top k Hit Rate for Various Algorithms at the 500th Respondant}
\label{table:at500}
\end{table}
 We show in Table \ref{table:at500}. After about 160 respondents, we've obtained an 80\% hit rate; which we wouldn't achieve with traditional Express MaxDiff until about the 800th respondent.  As shown, we can accomplish with about 260 respondents what it takes 1000 respondents to do with Express MaxDiff.  Either comparison shows that you can obtain equally good hit rates using the adaptive Bandit MaxDiff methodology with $\frac{1}{4}$ the sample size.\\
Figure \ref{fig:3hit} also shows the two Thompson Sampling approaches achieve nearly identical results. But we will show later this isn't the case if we use a misinformed start (Section Misinformed Start)\\
Figure \ref{fig:10hit} shows the results for Top-10 hit rate for our 120-item MaxDiff, which is a broader measure of success that requires obtaining a high degree of precision for an even broader reach of items than the Top-3 hit rate.  The conclusions are fairly similar as with Figure \ref{fig:3hit}.  It takes about 400 respondents to accomplish with Bandit MaxDiff what we accomplish with 1000 respondents under the non-adaptive express MaxDiff approach. This is supported in Table \ref{table:top3}\\


\section{Robustness tests}
\subsection{Misinformed Starts (When Early Responders Are Horribly Non-Representative)}
To this point, it seems like the adaptive Bandit MaxDiff approach using Thompson Sampling is the clear winner.  However, what would happen if the first 50 respondents we interviewed were actually not very representative of the average preferences for the sample?  What if we tried to throw Bandit MaxDiff off the scent?  In fact, let's consider a worst-case scenario: the first responders actually believe \textit{nearly the opposite} from the rest of the sample\\
For the simulations reported in Figure 6, the first 50 robotic respondents mimicked randomly drawn human vectors of utilities as before but were diabolically manipulated to behave as if the top 3 true items were actually nearly the worst in preference (we set the utilities for the top 3 true items for the population equal to the bottom 25th percentile utility item for each respondent).  After this misinformed start, the remaining respondents represented well-behaved respondents drawn using bootstrap sampling as before, with true individual-level preferences as given in the original dataset donated by Procter \& Gamble.\\
Our diabolical simulation of a misinformed start is worse than anything you would realistically see in practice, so it is a strong test of the robustness of the Bandit MaxDiff approach. This suggests robustness to non-stationarity in preference or self-selection of respondents during the sampling window. We created 50 misinforming early responders because in practice we are never guaranteed that the first responders represent a fair and representative draw from the population.  In fact, depending on how rapidly we invite a panel of respondents to take the survey, the first 50 respondents may share some atypical characteristics (e.g. anxious and available to take the survey at your launch time).  It would be a bad thing if the Thompson Sampling approach performed well in simulations with well-behaved respondents, but fell apart under more realistic conditions. \\
\begin{figure}
\caption{3 Hit Rate with 120 items with Misinformed Start}
\includegraphics[width=1\textwidth]{plots/3hitrate120show3mis.pdf}
\label{fig:3hitmis}
\end{figure}
We illustrate the robustness in Figure \ref{fig:3hitmis}. First, the Bandit MaxDiff $\epsilon$-diffuse TS approach is much better in the face of misinformed starts than the standard Bandit MaxDiff TS approach.  The more diffuse prior on the 10 out of 30 items within the split allows us to continue investigating the value of some lesser chosen items with enough frequency among later respondents, even if the prior respondents seem to have generally rejected them.\\
Second, even in the face of a misinformed start, the Bandit MaxDiff $\epsilon$-diffuse TS achieves equally good results as the standard Fixed Express MaxDiff without the misinformed start after 290 respondents. Finally, the conclusions were qualitatively similar when examining Top-10 hit rate.\\
\subsection{Changing exploration parameters}
We introduced two tuning parameters -- $\epsilon$ and $\delta$ -- into an algorithm that previously had no such parameters explicitly guiding exploration. To understand the impact these exploration parameters have and each one's effect on the outcome, we have to test their values empirically. \\
the Fixed Express MaxDiff is equivalent to $\epsilon$=1 and $\delta$=$0^+$, and greedy is equivalent to $\epsilon$=1 and $\delta$=$\infty$.  Our current resealts suggest there is an ``interior solution,'' as there are non-monotonic effects of the exploration parameters.
\subsection{Effect of increasing number of items}
\begin{table}
\begin{center}
\begin{tabular}{l | c | c | c}
 Respondent& Fixed Express &  TS $\epsilon$=$\frac{1}{3}$ $\delta$=10  &TS $\epsilon$=0 $\delta$=1 \\
\hline
100	&	22.33	&	44.33	&	47.67	\\
200	&	31	&	70	&	67	\\	
300	&	41.33	&	76	&	72	\\	
400	&	42.67	&	79	&	78.33	\\	
500	&	46.33	&	85.33	&	79.33	\\
600	&	48	&	89.33	&	82	\\	
700	&	53.67	&	88.67	&	85.67	\\
800	&	54.67	&	90.67	&	86.33	\\
900	&	55.67	&	91.33	&	88	\\
1000	&	59	&	91.33	&	89	\\	
\hline
\end{tabular}
\end{center}
\caption{Top 3 Hit Rate for Various Algorithms with 300 items}
\label{table:300top3}
\end{table}
Would the benefits of Bandit MaxDiff we observed with 120 items continue for 300 items?  While we didn't have a dataset of utilities from human respondents on 300 items, we did our best to generate such a data set by leveraging the 120-item data set Procter \& Gamble shared with us.  To generate preferences across an additional set of 180 items, we randomly combined pairs of existing items according to a randomly distributed weighting scheme, with additional random variation added.  The result was a 300-item MaxDiff data set based on the original preferences of the 981 respondents.\\
The advantages seen in the 120-item results are improved upon in the results with 300 items. Table \ref{table:300top3} shows the well-informed start results. The Bandit MaxDiff approach was 6x more efficient than the Fixed Express MaxDiff approach on the top-3 hit rate criterion.\\
\subsection{What about a smaller set of 40 Items?}
Our Bandit model has a great advantage over fixed designs for very large numbers of items, but what happens if we have a more traditional ``large'' MaxDiff list of 40 items. Using a random 40-item subset from our original set of 120 items, we reran our simulations. \\
%By reducing the number of items to 40 and changing the number of tasks to 12, the fixed express design can now show each item an average of 1.5 times per respondent, which is much less sparse than in larger item cases.\\
By reducing the number of items to 40, the Express MaxDiff design can now show each item to every other respondent on average, which is much more than in larger item cases.
Nevertheless, our results for Bandit MaxDiff are still better than traditional MaxDiff. We also still see a tremendous advantage in the case of a misinformed start.
\subsection{Using True Utility as a Generalization of Hit-Rate}
\begin{table}
\begin{center}
\begin{tabular}{c | c }
$S$& Percent True Utility \\
\hline
\{1,2,3\}& 1.0 \\
\{1,2,4\}&.973 \\
\{1,2,5\}&.948 \\
\{1,3,4\}&.934 \\
\{1,3,5\}&.910 \\
\{1,4,5\}&.882 \\
\{2,3,4\}&.922 \\
\{2,3,5\}&.897 \\
\{2,4,5\}&.870 \\
\{3,4,5\}&.831 \\
\hline
\end{tabular}
\end{center}
\caption{Percent true utility for Various $S$ for a Top 3 scheme from the Procter \& Gamble data}
\label{table:PTU}
\end{table}
One might like to differentiate between an algorithm puts the top 9 items and the 11th item in the 10 top and one puts the top 9 items and the 40th item in the 10 top. A measure that differentiates the two is Percent True Utility (PTU). \\
\textbf{Exponential Weighted Utility}: For a set $S$ the exponential weighted utility is \[\mu_S=\sum_{x \in S}b(x)=\sum_{x \in S}e^{u(x)}\]
\textbf{Percent True Utility of the Top 3}: The Percent True Utility (PTU) is the exponential weighted true utility of the top 3 items that the robotic respondents identified over the maximum exponential weighted true utility that could be attained with 3 items. Mathematical if $\tilde{S}$ is the top 3 items and $S$ is the current 3 top items identified robotic respondents then PTU is 
\[
\frac{\mu_S}{\mu_{\tilde{S}}}
\]
This can be see as a generalization of Hit-Rate. It has the advantage of differentiating as in the case above though one loses the ability to easy interpret the result. In Table \ref{table:PTU} we show the PTU for various subsets of the data.
\begin{figure}
\caption{Percent True Utility of the Top 20 with 120 items}
\includegraphics[width=1\textwidth]{plots/PTU120v20k20.pdf}
\label{fig:20util}
\end{figure}
Recall that for $k=20$ using the hit rate metric that Fixed Express, TS, and $\epsilon$ diffuse TS were on par with each other.
Using $k=20$ PTU, in Figure \ref{fig:20util} that $\epsilon$ diffuse TS has a slight edge over TS which are both better than Fixed Express. This means that both TS approaches put better ranked items in the top 20 then Fixed Express and our thus better even though the top 20 hit rate was about the same.
\subsection{Using Regret as a Stopping Rule}
From figure \ref{fig:20util} after 600 respondents TS finds the top 3 items most of the time. Thus it becomes inefficient to keep giving surveys after this point. In practice we do not know the True Utilities \textit{a priori} but we would still like to know when we should stop.\\
Taken from~\cite{scott2015multi} and~\cite{scott2010modern} for MAB, The value remaining in the experiment is the posterior distribution of $\frac{\mu_{S^*}-\mu_{S}}{\mu_{S}}$ where $\mu_{S^*}$ is the largest value of the exponential weighted utility and $\mu_{S}$ is the exponential weighted utility of the set that is most likely to be optimal, denoted $S$. This is constructed as follows, take $n$ Monte Carlo draws from $p(\mu|y_t)$. Let $\mu_{S^*}^{m}$ be the max exponential weighted utility of draw $m$ and $\mu_{S}^{m}$ be the utility using the draw $m$ using the set $S$. Let $\Delta^{m}=\frac{\mu^m_{S^*}-\mu^m_{S}}{\mu^m_{S}}$.\\
\begin{table}
\begin{center}
\begin{tabular}{l | c c c c c c c c}
Current & 1st &  2nd  &  3rd  &  4th &  5th & 6th & 7th &  8th \\
\hline
Draw 1 & 4.02 &  3.50 &  5.08 & 4.16&  4.22 & 4.41 & 3.65 &  3.27 \\
Draw 2 &4.18 & 4.72 & 3.49 & 3.48 & 3.63 & 3.60 & 3.56 &  3.70 \\
Draw 3 &4.81 & 5.23 & 5.04 &  3.96 &  4.17 & 4.37 &  3.58 & 2.99 \\ 
\end{tabular}
\end{center}
\caption{Draws of the exponetial utility of the items after 100 iterations}
\label{table:data}
\end{table}
\begin{figure}
\includegraphics[width=1\linewidth]{plots/valremhist.pdf}
\caption{Two histograms of $\Delta$. Left: After 100 iterations, the potential value remaining is .092. Right: After 220 iterations, the potential value remaining is .008}
\label{fig:data}
\end{figure}
As an example I took draws after respondent 100 in TS $\epsilon=.33$ $\delta=10$ for 120 items see table \ref{table:data} for draws of exponential weighted utility of a single item. I put the columns in current rank order and only show the top 8 for convenience. Then $S$ is the set containing what is currently ranked as the first, second and third ranked item. Then $\mu^1_{S}=4.02+3.50+5.08=12.6$ and $\mu_{S^*}^{1}=5.08+4.41+4.16=13.65$ So $\Delta^{1}=\frac{13.65-12.6}{12.6}=.083$. Likewise $\Delta^{2}=\frac{12.6-12.39}{12.39}=.017$ and $\Delta^{3}=\frac{15.08-15.08}{15.08}=0$ (Note $\Delta^m=0$ when the $S$ contains the top utilities). The histogram of $\Delta$ after 100 iterations and 220 iterations is shown in figure \ref{fig:data}. \\
 The `potential value remaining' (PVR) is the .95 quantile of the distribution $\Delta$. After 100 iterations the PVR was .092. Scott's way to interpret this number is ``we do not know what the utility of $S$ is, but whatever it is, a different set might beat it by as much as 9.2\%.''\\
 A good stopping rule is to stop when the PVR drops below a certain threshold, (We use .05 and .02). One advantage of this is it handles ties (two sets with close exponential weighted utility scores) really well. 
\begin{figure}
\caption{Potential Value Remaining of the Top 3 with 120 items on a semi log plot, the dashed lines are at .05 and .02}
\includegraphics[width=1\textwidth]{plots/3vr120show3.pdf}
\label{fig:3vr}
\end{figure}
\begin{figure}
\caption{Histogram of When Iterations Would Stop and PTU Using a .02 Stopping Rule for Top 3 for 120 Items}
\begin{subfigure}{.5\textwidth}
\caption{On which Respondent does Iteration Stops}
\includegraphics[width=1\textwidth]{plots/stop120items23.pdf}
\label{fig:3vrstop2}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Percent True Utility When Iteration Stops}
\includegraphics[width=1\textwidth]{plots/util120items23.pdf}
\label{fig:3vrutil2}
\end{subfigure}
\end{figure}
\begin{figure}
\caption{Histogram of When Iterations Would Stop and PTU Using a .05 Stopping Rule for Top 3 for 120 Items}
\begin{subfigure}{.5\textwidth}
\caption{On which Respondent does Iteration Stops}
\includegraphics[width=1\textwidth]{plots/stop120items53.pdf}
\label{fig:3vrstop5}
\end{subfigure}
\begin{subfigure}{.5\textwidth}
\caption{Percent True Utility When Iteration Stops}
\includegraphics[width=1\textwidth]{plots/util120items53.pdf}
\label{fig:3vrutil5}
\end{subfigure}
\end{figure}
Figure \ref{fig:3vr} shows the average PVR on a log scale. Figures \ref{fig:3vrstop2} and \ref{fig:3vrstop5} show on which respondent the iterations' PVR would reach .02 and .05 respectively (with the max being 1020)  and Figures \ref{fig:3vrutil2} and \ref{fig:3vrutil5} shows what the Percent True Utility would be at those points.\\
\begin{table}
\begin{center}
\begin{tabular}{l | c | c | c }
 & Fixed Express &  TS $\epsilon$=$\frac{1}{3}$ $\delta$=10  &TS $\epsilon$=0 $\delta$=1  \\
\hline
120 items&&&\\
Mean Stop&  980.8 &  440.6 &  606.0 \\
Mean PTU& .976 & .997 & .996 \\
\hline
300 items&&&\\
Mean Stop& 1020.0 &  674.2 &  929.4\\
Mean PTU& .904 & .992 & .988 \\
\hline
\end{tabular}
\end{center}
\caption{Mean respondent each algorithm would stop at and the mean percent true utility (PTU) for stopping when the PVR reaches .02}
\label{table:stop2}
\end{table}
\begin{table}
\begin{center}
\begin{tabular}{l | c | c | c }

 & Fixed Express &  TS $\epsilon$=$\frac{1}{3}$ $\delta$=10  &TS $\epsilon$=0 $\delta$=1 \\
\hline
120 items&&&\\
Mean Stop& 661.6 &  214.0 & 243.8 \\
Mean PTU& .966 & .984 & .986 \\
\hline
300 items&&&\\
Mean Stop& 1001.2 &  334.6 &  526.8\\
Mean PTU& .904 & .970 & .980 \\
\hline
\end{tabular}
\end{center}
\caption{Mean respondent each algorithm would stop at and the mean percent true utility (PTU) for stopping when the PVR reaches .05}
\label{table:stop5}
\end{table}
In tables \ref{table:stop2} and \ref{table:stop5} we see the means of the when the iterations would stop and percent true utility at those points for 120 and 300 items.\\ Another advantage we see from this for Bandit MaxDiff $\epsilon$-diffuse TS approach is that on average it stops sooner that standard TS with about the same performance. This advantage grows with the number of items. Both Bandit MaxDiff TS stop much sooner and preform much better then Fixed Express. The choice between a 0.05 stopping rule or a 0.02 stopping rule is a trade-off between the having to survey extra respondents and getting the extra utility.
%\subsection{Spearman (Rank Ordering) Correlation}
%Both hit rate and percent true utility are invariant under the ordering of the $k$ items. As a final metric we looked at the Spearmen Correlation of the true top $k$. This measures how well the algorithm's ranking of the true top $k$ items matches with the truth. For example if the 
\subsection{Asking for Bests instead of Best-Worst}
Because a key assumption for using the Bandit MaxDiff approach is that the researcher is mainly interested in identifying the top few items, we wondered about the value of spending time asking respondents to identify the worst item within each MaxDiff set.  What would happen if we asked our robotic respondents only to select the best item within each set?  The results somewhat surprised us.  The value of asking respondents to indicate both best and worst within each set more than compensated for the 40\% additional effort we suppose these ``worst'' questions add to the total interview time when interviewing human respondents.
In a five item set (A,B,C,D and E) there are 10 possible 2-way comparisons. If we assume A is preferred to B and B is preferred to C and so on, then asking about only the best item will let us know A$>$B, A$>$C, A$>$D and A$>$E (4/10 comparisons).  By asking about worsts as well, for only one additional question we also add B$>$E, C$>$E, and D$>$E (7/10 comparisons), leaving only the order relationship between B, C, and D unknown.\\
The case of asking for bests when all respondents have same preferences that turns into the marked-bandit problem in ~\cite{simchowitz2016best}. In that paper the authors give different algorithms for pulling the arms and upper and lower bounds on how many queries it takes to identify the top $k$ items with high probability. 
\subsection{What about Double Adaptivity?}
In 2006, one of the authors presented a paper on Adaptive MaxDiff that featured within-respondent adaptation ~\cite{orme2006adaptive} rather than what we have shown here in Bandit MaxDiff based on Thompson Sampling, which is an across-respondent adaptive approach.  For the within-respondent adaptive procedure, items that a respondent indicates are worst are dropped from further consideration by that same respondent through a round-robin tournament until eventually that respondent's best item is identified.  We thought adding this additional layer of within-respondent adaptivity on top of the Bandit MaxDiff approach could additionally lift its performance.  To our surprise, this double-adaptive approach actually performed worse than Bandit MaxDiff alone in terms of hit rates for the globally best 3 or 10 items for the sample.  After some head-scratching (and much code checking), we determined that the lack of improvement was due to degree of heterogeneity across the robotic respondents.  For example, if we are interviewing a respondent who doesn't agree much with the overall population regarding which are the top items, it is detrimental to allow that respondent to drop from further consideration (due to judging them worst) what actually are among the globally most preferred items.  It serves the greater good for each respondent to spend increased effort judging among the items that previous respondents on average have judged as potentially best.
\subsection{Drawing from the Asymptotic Distribution (add MCMC?)}
As an alternative to bayesian bootstraping to get draws from the posterior, one can sample from asymptotic distribution via MLE implied by the estimated mean and standard errors, sampling $u \sim N(\theta,H^{-1})$, where $\theta$ is the minimizer of the negative loglikehood and $H$ is the Hessian of the negative loglikehood evaluated at $\theta$. The sampling methods preformed about the same using draws from either bayesian bootstraping or the asymptotic distribution.

\subsection{Other Sampling Schemes}
%\subsection{Choosing Items to Minimize Misclassification}
%\subsection{What about when $k$ is close to $L$?}
In ~\cite{toubia2007adaptive}, which also looks at finding the top $k$ items in a list they take the items, their ``utility'' (referred to as score) for there top preforming algorithm is the probability that a top item should be a bottom item and that a bottom item should be a top item. Then they perturb each score with a random normal variable that decreases in variance over respondents. They take the items the highest score to be in the next survey. Likewise we could also choose our $L$ items to be the next survey this way instead of taking the top $L$ items. For example for finding the top 3, to find the score of a top item you sample from the posterior distribution $N$ times (where $N$ is large) and the score is the number of samples below the estimated utility score of the current rank 3 item over $N$ and for a bottom item it is the number of samples above the estimated utility score of the forth ranked item over $N$.\\
In ~\cite{russo2016simple}, the author analyses different Bayesian algorithms for identifying the best arm. One point he makes is that Thompson Sampling is not effective for purely exploration problems. TS leads to oversampling the top item and not getting good enough estimates the second best items to conclude with high enough confidence that the top item is the top item. He gives a variation Top-Two Thompson Sampling which is better for the MAB problem. This is choosing the best arm with probability $\beta$ and choosing the second best arm with probability $1-\beta$. One way to implement this in MaxDiff surveys is top pick the top 30 items and for each item with rank $i$ with probability $1-\beta$ replace that item with the rank $i+30$ ranked item.\\
While our problem is purely exploration, the nature of MaxDiff survey causes us to show $L$ items to every respondent, putting a limit on how much we are sampling the each item. As a result when $k$ is less $L$ we do not run in to the same estimation problems as in MAB problem. Though as we saw in PVR, adding randomness can improve the speed of parameter estimates in the same spirit as Russo's paper.\\
%\subsection{Thompson Sampling for purely exploration problems}

%\begin{figure}
%\caption{30 Hit Rate with 120 items}
%\includegraphics[width=1\textwidth]{plots/30hitrate120show3.pdf}
%\label{fig:30hit}
%\end{figure}
%Consider \ref{fig:30hit}, while MAB methods still gives a edge over fixed sparse the gains are much less. One reason is that TS sampling will  
%at is considered in ~\cite{toubia2007adaptive}, but for a different type of survey. Instead of showing a series of items and ask respondents to pick the best and worst from a list, the survey shows the respondents one item and asked to rate it using the binary choice good or bad. With each query they get information on one item. As a result the authors show fewer items to each respondent. Similar to our approach, the authors devise adaptive approaches that use previous ratings to decide the what items to show the next respondent.  Unlike our approach, they sample items more that are likely to be misclassified by looking at items close to the threshold between top items and bottom items (not top items).\\
%It is interesting to note that their best preforming solution is similar in spirit to our method of $\epsilon$-Diffuse TS. In implementation their method uses a beta distribution and can calculate their score directly instead of taking a sample. They then add noise from a normal distribution to each of the scores and show the top scoring items to the next respondent. Despite the differences, the similarity between their best method and is $\epsilon$-Diffuse TS striking.
\subsection{What about Sparse MaxDiff vs. Express MaxDiff?}
Wirth and Wolfrath compared non-adaptive Sparse MaxDiff and Express MaxDiff in their 2012 paper at the Sawtooth Software Conference ~\cite{wirth2012largeset}. 
\textbf{Fixed Sparse MaxDiff}: we showed each item to each respondent an equal number of times (if possible).  With 120 items, 18 sets, and 5 items per set each item appeared on average $\frac{18*5}{120} = 0.75$ times per respondent. \\
 We compared the results using our simulation and found a modest edge in performance for Express MaxDiff (Sparse ending at 85.6\% for top 10 hit rate and express ending at 87.7\%).
\section{Conclusions and Future Research}
Our results suggest that if your main purpose in using large item lists in MaxDiff is to identify the top items for the population (not individual-level estimates), then adaptive Bandit MaxDiff approaches can be 4x more efficient than standard Express MaxDiff designs.  You are potentially wasting 75 cents of each dollar spent on data collection by not using Bandit MaxDiff.\\
Bandit MaxDiff leverages information from prior respondents to show more effective tradeoffs to later respondents (tending to oversample the stars, based on the Thompson Sampling mechanism).  Even in the face of diabolically imposed misinformed starts (horribly unrepresentative first responders), the Bandit MaxDiff approach with our 20/10 split is extremely robust and self-correcting.\\
Although our simulations involve 120-item and 300-item tests, we expect that even greater efficiency gains than 4x (compared to standard Express MaxDiff designs) may occur with 500-item (or more) MaxDiff studies.  For studies using 40 respondents, our simulation showed a 2x advantage in efficiency over fixed MaxDiff designs. Though not as dramatic, this is still a sizable boost.\\
Future research should test our findings using human respondents.  Using an adaptive process that focuses on comparing best items may result in a more cognitively difficult task than a standard level-balanced, near-orthogonal approach.  The greater expected within-set utility balance may lead to higher response error which may counteract some of the benefits of the Bandit adaptive approach.  However, based on previous research ~\cite{orme2006adaptive} that employed within-respondent adaptivity, the additional degree of difficulty that the Bandit adaptive approach could impose upon individual respondents (owing to utility balance) would probably not counteract the lion share of the benefits we've demonstrated using simulated respondents.\\

This paper also introduces a framework that links conjoint methods and multi-armed bandit methods. Much like adaptive conjoint methods began with aggregate adaptation and then progressed to individual-level adaptive, so we propose an aggregate adaptive approach. But future could explore methods using fully heterogeneous models, and adapting within each individual. A partially pooled model will be useful here, as it is with other adaptive conjoint methods. \\

Our method relates to existing adaptive conjoint just as M-efficiency criterion is to D-efficiency.  Unlike M-efficiency designs where the researcher decides the managerial weight of different factors a priori, we know which of the items should receive more weight. Instead, that is exactly what we want to learn actively. \\


We should note that as of this article's publication date, Sawtooth Software does not offer Bandit MaxDiff as a commercial tool.  Sawtooth Software may perhaps one day soon offer Bandit MaxDiff as an option within its commercially available MaxDiff software.  As for the authors, we look forward to this possibility as we've been especially impressed by the potential cost savings and increased accuracy!








% Acknowledgments here
\ACKNOWLEDGMENT{%
% Enter the text of acknowledgments here
}% Leave this (end of acknowledgment)


% Appendix here
% Options are (1) APPENDIX (with or without general title) or 
%             (2) APPENDICES (if it has more than one unrelated sections)
% Outcomment the appropriate case if necessary
%
% \begin{APPENDIX}{<Title of the Appendix>}
% \end{APPENDIX}
%
%   or 
%
% \begin{APPENDICES}
% \section{<Title of Section A>}
% \section{<Title of Section B>}
% etc
% \end{APPENDICES}

% References here (outcomment the appropriate case) 

% CASE 1: BiBTeX used to constantly update the references 
%   (while the paper is being written).
\bibliographystyle{informs2014} % outcomment this and next line in Case 1
\bibliography{source} % if more than one, comma separated

% CASE 2: BiBTeX used to generate mypaper.bbl (to be further fine tuned)
%\input{mypaper.bbl} % outcomment this line in Case 2

\end{document}


